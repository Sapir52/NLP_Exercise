{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1eca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deplacy stanza\n",
    "import stanza\n",
    "#----------------------\n",
    "#!pip install deplacy trankit transformers\n",
    "import trankit\n",
    "#----------------------\n",
    "#!pip install deplacy spacy-udpipe\n",
    "import spacy_udpipe\n",
    "#----------------------\n",
    "#!pip install deplacy spacy_jptdp\n",
    "import spacy_jptdp\n",
    "#----------------------\n",
    "#!pip install --index-url https://pypi.clarin-pl.eu/simple deplacy combo\n",
    "#!pip install combo\n",
    "import combo\n",
    "#import combo.predict\n",
    "#----------------------\n",
    "#!pip install deplacy camphr en-udify@https://github.com/PKSHATechnology-Research/camphr_models/releases/download/0.7.0/en_udify-0.7.tar.gz\n",
    "import pkg_resources,imp\n",
    "imp.reload(pkg_resources)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ccfe3a",
   "metadata": {},
   "source": [
    "What is the purpose of syntactic analysis?\n",
    "Its purpose is to understand the structure of input text, from the smallest basic symbols, all the way to sentences, and then derive logical meaning from it.\n",
    "\n",
    "Syntactic analysis is an extremely important aspect of natural language processing (NLP) because it assists in figuring out the grammatical meaning of any sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f267ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 02:23:41,467 | INFO | textcleaner.py:12 | <module> | 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "# Import file\n",
    "import codecs\n",
    "# Stop words\n",
    "#!pip install advertools\n",
    "import advertools as adv\n",
    "# Presenting a semantic analysis\n",
    "import deplacy\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "# Text samerization\n",
    "#!pip install summa\n",
    "import summa\n",
    "from summa import summarizer\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7213672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 02:28:08 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2022-07-15 02:28:08,569 | INFO | core.py:112 | __init__ | Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2022-07-15 02:28:08 INFO: Use device: cpu\n",
      "2022-07-15 02:28:08,572 | INFO | core.py:123 | __init__ | Use device: cpu\n",
      "2022-07-15 02:28:08 INFO: Loading: tokenize\n",
      "2022-07-15 02:28:08,573 | INFO | core.py:129 | __init__ | Loading: tokenize\n",
      "2022-07-15 02:28:08 INFO: Loading: mwt\n",
      "2022-07-15 02:28:08,584 | INFO | core.py:129 | __init__ | Loading: mwt\n",
      "2022-07-15 02:28:08 INFO: Loading: pos\n",
      "2022-07-15 02:28:08,605 | INFO | core.py:129 | __init__ | Loading: pos\n",
      "2022-07-15 02:28:08 INFO: Loading: lemma\n",
      "2022-07-15 02:28:08,936 | INFO | core.py:129 | __init__ | Loading: lemma\n",
      "2022-07-15 02:28:08 INFO: Loading: depparse\n",
      "2022-07-15 02:28:08,990 | INFO | core.py:129 | __init__ | Loading: depparse\n",
      "2022-07-15 02:28:09 INFO: Done loading processors!\n",
      "2022-07-15 02:28:09,641 | INFO | core.py:179 | __init__ | Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     כאב בכה משפחה תדע צער\n",
      "1                                   איש יקר\n",
      "2                            כבוד מון הצלחה\n",
      "3    תל חי רובי עצב קרן אור תקוה נשיא ישראל\n",
      "4                       נקי כף בר לבב הצלחה\n",
      "Name: tokens, dtype: object\n",
      "Text summary:\n",
      " 3                תל חי רובי עצב קרן אור תקוה נשיא ישראל\n",
      "7     כבוד נשיא התאים עולם מלהי נשיא מדינה ישראל ישר...\n",
      "8                 כבוד אדון נשיא איש ימין חשב אהבה ניצח\n",
      "17                     ווא ריגש עזרה ׳ נשיא מדינה ישראל\n",
      "27    כבוד נשיא בא טוב שמח פגישה עתידי אבו מאזן בקש ...\n",
      "29                              כבוד נשיא כבוד תמך מילה\n",
      "30                                        כוח כבוד נשיא\n",
      "45                       חיוך כבש מדינה ישראל כבוד נשיא\n",
      "46                      נשיא מדינה ישראל כלבבי יישר כוח\n",
      "47                 מילה כדורבן גא רובי נשיא מדינה ישראל\n",
      "66                        יח נשיא חדש תפארת מדינה ישראל\n",
      "72                בריאות כבוד נשיא הלך לנכד משפחה ישראל\n",
      "74    פוסט ראה ראוי תואר כבוד נשיא מדינה ראובן רובי ...\n",
      "79      כבוד נשיא זל טוב גאה העריץ שנה None הצלחה תפקיד\n",
      "83                                      אהב נשיא ריבלין\n",
      "84    אדון נשיא ערב נשמע קינה מדינה ישראל שא התחיל ט...\n",
      "86    כבוד נשיא יקר זמן קצר כהונה התבטאות עשה כבוד ת...\n",
      "88    הצלחה שמחה רב מחת שמע נשיא בא מדינה ישראל יְבָ...\n",
      "93                     רכות מון הצלחה בית נשיא אהב רובי\n",
      "99                 כבוד נשיא ראובן ריבלין תדה נשיא כמוך\n",
      "keywords:\n",
      " נשיא ישראל\n",
      "כבוד\n",
      "הצלחה\n",
      "רובי\n",
      "משפחה\n",
      "מדינה\n",
      "יום\n",
      "ריבלין\n",
      "איש יקר\n",
      "אדון\n",
      "אהב\n",
      "תדה\n",
      "בא טוב\n",
      "חשוב עשה\n",
      "הלך\n",
      "חשב אהבה\n",
      "האמין\n",
      "בית\n",
      "איחל שנה\n",
      "ב\n",
      "אדם\n",
      "הגיע\n",
      "שמח\n",
      "בחירה נכון\n",
      "אמת\n",
      "כוח\n",
      "יהודי שבת שלום\n",
      "שבוע תפקיד\n",
      "ידע\n",
      "בן\n",
      "גא\n",
      "שתיקה\n",
      "זמן\n",
      "ראובן\n",
      "ברוך\n",
      "צדק\n",
      "נתן\n",
      "פוסט\n",
      "ריגש\n",
      "חייב\n",
      "מקסים\n",
      "ראה\n",
      "נשיאות\n",
      "שמע\n",
      "כתב\n",
      "הנשיא\n",
      "חיוך\n",
      "ת\n",
      "התאים עולם\n",
      "מעשה\n",
      "ראוי\n",
      "אמן\n",
      "התחיל\n",
      "תמך\n",
      "רב\n",
      "שמחה\n",
      "ישר\n",
      "כאפות חנך\n",
      "ארתעה\n"
     ]
    }
   ],
   "source": [
    "class HebrewDataProcessing():\n",
    "    def load_data(self, file_name):\n",
    "        # Load dataSet\n",
    "        data = list(codecs.open(file_name, 'r', 'utf-8').readlines())\n",
    "        df =pd.DataFrame({'text':data})\n",
    "        return df\n",
    "    \n",
    "    def get_syntactic_analysis(self, type, df):\n",
    "        # Select a library type to preform data syntax analysis\n",
    "        if type=='stanza':\n",
    "            #stanza.download('he') \n",
    "            nlp=stanza.Pipeline(\"he\")\n",
    "        elif type=='trankit':\n",
    "            nlp=trankit.Pipeline(\"hebrew\")\n",
    "        elif type=='spacy-udpipe':\n",
    "            #spacy_udpipe.download(\"he\")\n",
    "            nlp=spacy_udpipe.load(\"he\")\n",
    "        elif type=='spacy-jptdp':\n",
    "            nlp=spacy_jptdp.load(\"he_htb\")\n",
    "        elif type == 'combo-pytorch':\n",
    "            nlp=combo.predict.COMBO.from_pretrained(\"hebrew-ud27\")\n",
    "        elif type=='camphr-udify':\n",
    "            nlp=spacy.load(\"en_udify\")\n",
    "        #doc=nlp(str(df))\n",
    "        #deplacy.render(doc,WordRight=True)\n",
    "        #deplacy.serve(doc,port=None,RtoL=True)\n",
    "        return nlp\n",
    "    \n",
    "    def get_tokens(self, doc):\n",
    "        # list of upos\n",
    "        upos =[\"CCONJ\",\"PUNCT\", \"ADP\", \"PRON\", \"DET\", \"SCONJ\", \"NUM\", \"ADV\"]\n",
    "        # Remove specific upos\n",
    "        list_of_tokens = [str(word.lemma) for sent in doc.sentences for word in sent.words  if word.upos not in upos]\n",
    "        return [list_of_tokens] \n",
    "\n",
    "\n",
    "    def data_clearing(self, df):\n",
    "        # remove urls\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0]) \n",
    "        # remove numbers and eng words \n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x:re.sub('[0-9a-zA-Z+]',' ', str(x)))\n",
    "        # Removing html tags and amoji\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x:re.sub(r'<[^>]*>', '', str(x))) \n",
    "        amoji ='[(\\U0001F600-\\U0001F92F|\\U0001F300-\\U0001F5FF|\\U0001F680-\\U0001F6FF|\\U0001F190-\\U0001F1FF|\\U00002702-\\U000027B0|\\U0001F926-\\U0001FA9F|\\u200d|\\u2640-\\u2642|\\u2600-\\u2B55|\\u23cf|\\u23e9|\\u231a|\\ufe0f)]'\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda i:re.sub(amoji+'+','', str(i))) \n",
    "        #---------------------------------------\n",
    "        # tokenize\n",
    "        nlp = self.get_syntactic_analysis('stanza', df[\"text\"])\n",
    "        df_nlp = df[\"text\"].apply(nlp)\n",
    "        df_nlp = df_nlp.apply(lambda r: pd.Series(self.get_tokens(r), index=['tokens']))\n",
    "        #---------------------------------------\n",
    "        # list of stopwords by the spaCy package\n",
    "        word_tokens = list(adv.stopwords['hebrew']) \n",
    "        # remove stopwords\n",
    "        df_nlp['tokens']=df_nlp['tokens'].apply(lambda x: [word_token for word_token in x if word_token not in word_tokens])\n",
    "        #---------------------------------------\n",
    "        # convert list to string\n",
    "        df_nlp['tokens'] = (df_nlp['tokens'].transform(lambda x: \" \".join(map(str,x))))        \n",
    "        return df_nlp['tokens']\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    obj = HebrewDataProcessing()\n",
    "    #Read dataset\n",
    "    df= obj.load_data('data/hebrew_text.tsv')\n",
    "    df=df.head(100)\n",
    "    df_nlp = obj.data_clearing(df)\n",
    "    df_nlp.to_csv('data/preprocessing_hebrew.csv', index = False, header=False)\n",
    "    df=df.head(100)\n",
    "    print(df_nlp.head())\n",
    "    def hebrow_summarizer(str_text):\n",
    "        print('Text summary:\\n', summarizer.summarize(str_text))\n",
    "        print('keywords:\\n', keywords.keywords(str_text))\n",
    "        \n",
    "    str_text = df_nlp.to_string()\n",
    "    hebrow_summarizer(str_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64285464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
