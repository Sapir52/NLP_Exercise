{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1eca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deplacy stanza\n",
    "import stanza\n",
    "#----------------------\n",
    "#!pip install deplacy trankit transformers\n",
    "import trankit\n",
    "#----------------------\n",
    "#!pip install deplacy spacy-udpipe\n",
    "import spacy_udpipe\n",
    "#----------------------\n",
    "#!pip install deplacy spacy_jptdp\n",
    "import spacy_jptdp\n",
    "#----------------------\n",
    "#!pip install --index-url https://pypi.clarin-pl.eu/simple deplacy combo\n",
    "#!pip install combo\n",
    "import combo\n",
    "#import combo.predict\n",
    "#----------------------\n",
    "#!pip install deplacy camphr en-udify@https://github.com/PKSHATechnology-Research/camphr_models/releases/download/0.7.0/en_udify-0.7.tar.gz\n",
    "import pkg_resources,imp\n",
    "imp.reload(pkg_resources)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ccfe3a",
   "metadata": {},
   "source": [
    "What is the purpose of syntactic analysis?\n",
    "Its purpose is to understand the structure of input text, from the smallest basic symbols, all the way to sentences, and then derive logical meaning from it.\n",
    "\n",
    "Syntactic analysis is an extremely important aspect of natural language processing (NLP) because it assists in figuring out the grammatical meaning of any sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f267ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 02:55:11,250 | INFO | textcleaner.py:12 | <module> | 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "# Import file\n",
    "import codecs\n",
    "# Stop words\n",
    "#!pip install advertools\n",
    "import advertools as adv\n",
    "# Presenting a semantic analysis\n",
    "import deplacy\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "# Text samerization\n",
    "#!pip install summa\n",
    "import summa\n",
    "from summa import summarizer\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7213672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 02:56:46 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2022-07-15 02:56:46,843 | INFO | core.py:112 | __init__ | Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2022-07-15 02:56:46 INFO: Use device: cpu\n",
      "2022-07-15 02:56:46,845 | INFO | core.py:123 | __init__ | Use device: cpu\n",
      "2022-07-15 02:56:46 INFO: Loading: tokenize\n",
      "2022-07-15 02:56:46,847 | INFO | core.py:129 | __init__ | Loading: tokenize\n",
      "2022-07-15 02:56:46 INFO: Loading: mwt\n",
      "2022-07-15 02:56:46,856 | INFO | core.py:129 | __init__ | Loading: mwt\n",
      "2022-07-15 02:56:46 INFO: Loading: pos\n",
      "2022-07-15 02:56:46,874 | INFO | core.py:129 | __init__ | Loading: pos\n",
      "2022-07-15 02:56:47 INFO: Loading: lemma\n",
      "2022-07-15 02:56:47,259 | INFO | core.py:129 | __init__ | Loading: lemma\n",
      "2022-07-15 02:56:47 INFO: Loading: depparse\n",
      "2022-07-15 02:56:47,313 | INFO | core.py:129 | __init__ | Loading: depparse\n",
      "2022-07-15 02:56:47 INFO: Done loading processors!\n",
      "2022-07-15 02:56:47,969 | INFO | core.py:179 | __init__ | Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      כאב בכה משפחה תדע צער\n",
      "1                                    איש יקר\n",
      "2                             כבוד מון הצלחה\n",
      "3    תל חי רובי עצב קרן אור תקוה נשיא בישראל\n",
      "4                        נקי כף בר לבב הצלחה\n",
      "Name: tokens, dtype: object\n",
      "Text summary:\n",
      " 7     כבוד נשיא התאים עולם מלהי נשיא מדינה ישראל ישר...\n",
      "8             כבוד הלך אדון נשיא איש ימין חשב אהבה ניצח\n",
      "11                             אנה חשב הגיע נשיא נורמלי\n",
      "17                       ווא ריגש עזרה נשיא מדינה ישראל\n",
      "25                  תגובה נכתב עבודה עשה תור נשיא הצלחה\n",
      "27    כבוד נשיא בא עלה טוב פגישה עתידי אבו מאזן בקש ...\n",
      "29                              כבוד נשיא כבוד תמך מילה\n",
      "45                      חיוך כבש מדינה ישראל כבוד הנשיא\n",
      "46                      נשיא מדינה ישראל כלבבי יישר כוח\n",
      "47                 מילה כדורבן גא רובי נשיא מדינה ישראל\n",
      "keywords:\n",
      " נשיא\n",
      "כבוד\n",
      "הצלחה\n",
      "רובי\n",
      "איש\n",
      "חשב\n",
      "טוב\n",
      "מדינה ישראל\n",
      "עשה\n",
      "הנשיא\n",
      "הגיע\n",
      "נכון\n",
      "אדון\n",
      "משפחה\n",
      "שתיקה\n",
      "בת\n",
      "שנה\n",
      "תדה\n",
      "ריבלין היקר\n",
      "יום\n",
      "מעשה\n",
      "אהב\n",
      "נשיאות\n",
      "חיוך\n",
      "כוח\n",
      "שבוע תפקיד\n",
      "שמח\n",
      "השיף\n",
      "מילה\n",
      "יישר\n",
      "זמן פרסם פוסט כלה טר מסכנה\n",
      "אמת עזר\n"
     ]
    }
   ],
   "source": [
    "class HebrewDataProcessing():\n",
    "    def load_data(self, file_name):\n",
    "        # Load dataSet\n",
    "        data = list(codecs.open(file_name, 'r', 'utf-8').readlines())\n",
    "        df =pd.DataFrame({'text':data})\n",
    "        return df\n",
    "    \n",
    "    def get_syntactic_analysis(self, type, df):\n",
    "        # Select a library type to preform data syntax analysis\n",
    "        if type=='stanza':\n",
    "            #stanza.download('he') \n",
    "            nlp=stanza.Pipeline(\"he\")\n",
    "        elif type=='trankit':\n",
    "            nlp=trankit.Pipeline(\"hebrew\")\n",
    "        elif type=='spacy-udpipe':\n",
    "            #spacy_udpipe.download(\"he\")\n",
    "            nlp=spacy_udpipe.load(\"he\")\n",
    "        elif type=='spacy-jptdp':\n",
    "            nlp=spacy_jptdp.load(\"he_htb\")\n",
    "        elif type == 'combo-pytorch':\n",
    "            nlp=combo.predict.COMBO.from_pretrained(\"hebrew-ud27\")\n",
    "        elif type=='camphr-udify':\n",
    "            nlp=spacy.load(\"en_udify\")\n",
    "        #doc=nlp(str(df))\n",
    "        #deplacy.render(doc,WordRight=True)\n",
    "        #deplacy.serve(doc,port=None,RtoL=True)\n",
    "        return nlp\n",
    "    \n",
    "    def get_tokens(self, doc):\n",
    "        # list of upos\n",
    "        upos =[\"CCONJ\",\"PUNCT\", \"ADP\", \"PRON\", \"DET\", \"SCONJ\", \"NUM\", \"ADV\"]\n",
    "        # Remove specific upos\n",
    "        list_of_tokens = [str(word.lemma) for sent in doc.sentences for word in sent.words  if word.upos not in upos]\n",
    "        return [list_of_tokens] \n",
    "\n",
    "\n",
    "    def data_clearing(self, df):\n",
    "        # remove urls\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0]) \n",
    "        # remove numbers and eng words \n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x:re.sub('[\\W+|0-9\\n]',' ', str(x)))\n",
    "        # Removing html tags and amoji\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x:re.sub(r'<[^>]*>', '', str(x))) \n",
    "        amoji ='[(\\U0001F600-\\U0001F92F|\\U0001F300-\\U0001F5FF|\\U0001F680-\\U0001F6FF|\\U0001F190-\\U0001F1FF|\\U00002702-\\U000027B0|\\U0001F926-\\U0001FA9F|\\u200d|\\u2640-\\u2642|\\u2600-\\u2B55|\\u23cf|\\u23e9|\\u231a|\\ufe0f)]'\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda i:re.sub(amoji+'+','', str(i))) \n",
    "        #---------------------------------------\n",
    "        # tokenize\n",
    "        nlp = self.get_syntactic_analysis('stanza', df[\"text\"])\n",
    "        df_nlp = df[\"text\"].apply(nlp)\n",
    "        df_nlp = df_nlp.apply(lambda r: pd.Series(self.get_tokens(r), index=['tokens']))\n",
    "        #---------------------------------------\n",
    "        # list of stopwords by the spaCy package\n",
    "        word_tokens = list(adv.stopwords['hebrew']) \n",
    "        # remove stopwords\n",
    "        df_nlp['tokens']=df_nlp['tokens'].apply(lambda x: [word_token for word_token in x if word_token not in word_tokens])\n",
    "        #---------------------------------------\n",
    "        # convert list to string\n",
    "        df_nlp['tokens'] = (df_nlp['tokens'].transform(lambda x: \" \".join(map(str,x))))        \n",
    "        return df_nlp['tokens']\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    obj = HebrewDataProcessing()\n",
    "    #Read dataset\n",
    "    df= obj.load_data('data/hebrew_text.tsv')\n",
    "    df_nlp = obj.data_clearing(df)\n",
    "    df_nlp.to_csv('data/preprocessing_hebrew.csv', index = False, header=False)\n",
    "    df_nlp=df_nlp.head(50)\n",
    "    print(df_nlp.head())\n",
    "    def hebrow_summarizer(str_text):\n",
    "        print('Text summary:\\n', summarizer.summarize(str_text))\n",
    "        print('keywords:\\n', keywords.keywords(str_text))\n",
    "        \n",
    "    str_text = df_nlp.to_string()\n",
    "    hebrow_summarizer(str_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64285464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
