{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1eca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deplacy stanza\n",
    "import stanza\n",
    "#----------------------\n",
    "#!pip install deplacy trankit transformers\n",
    "import trankit\n",
    "#----------------------\n",
    "#!pip install deplacy spacy-udpipe\n",
    "import spacy_udpipe\n",
    "#----------------------\n",
    "#!pip install deplacy spacy_jptdp\n",
    "import spacy_jptdp\n",
    "#----------------------\n",
    "#!pip install --index-url https://pypi.clarin-pl.eu/simple deplacy combo\n",
    "#!pip install combo\n",
    "import combo\n",
    "#import combo.predict\n",
    "#----------------------\n",
    "#!pip install deplacy camphr en-udify@https://github.com/PKSHATechnology-Research/camphr_models/releases/download/0.7.0/en_udify-0.7.tar.gz\n",
    "import pkg_resources,imp\n",
    "imp.reload(pkg_resources)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f267ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:45:32,141 | INFO | textcleaner.py:12 | <module> | 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "# Import file\n",
    "import codecs\n",
    "# Stop words\n",
    "#!pip install advertools\n",
    "import advertools as adv\n",
    "# Presenting a semantic analysis\n",
    "import deplacy\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "# Text samerization\n",
    "#!pip install summa\n",
    "import summa\n",
    "from summa import summarizer\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f588df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HebrewDataProcessing():\n",
    "    def load_data(self, file_name):\n",
    "        # Load dataSet\n",
    "        data = list(codecs.open(file_name, 'r', 'utf-8').readlines())\n",
    "        df =pd.DataFrame({'text':data})\n",
    "        return df\n",
    "    \n",
    "    def get_syntactic_analysis(self, type, df):\n",
    "        # Select a library type to preform data syntax analysis\n",
    "        if type=='stanza':\n",
    "            #stanza.download('he') \n",
    "            nlp=stanza.Pipeline(\"he\")\n",
    "        elif type=='trankit':\n",
    "            nlp=trankit.Pipeline(\"hebrew\")\n",
    "        elif type=='spacy-udpipe':\n",
    "            #spacy_udpipe.download(\"he\")\n",
    "            nlp=spacy_udpipe.load(\"he\")\n",
    "        elif type=='spacy-jptdp':\n",
    "            nlp=spacy_jptdp.load(\"he_htb\")\n",
    "        elif type == 'combo-pytorch':\n",
    "            nlp=combo.predict.COMBO.from_pretrained(\"hebrew-ud27\")\n",
    "        elif type=='camphr-udify':\n",
    "            nlp=spacy.load(\"en_udify\")\n",
    "        doc=nlp(str(df))\n",
    "        #deplacy.render(doc,WordRight=True)\n",
    "        #deplacy.serve(doc,port=None,RtoL=True)\n",
    "        return nlp\n",
    "    \n",
    "    def remove_urls(self, hebrew_text):\n",
    "        # Removing urls\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+', '', hebrew_text)\n",
    "\n",
    "    def get_tokens(self, doc):\n",
    "        # list of upos\n",
    "        upos =[\"CCONJ\",\"PUNCT\", \"ADP\", \"PRON\", \"DET\", \"SCONJ\", \"NUM\", \"ADV\"]\n",
    "        # Remove specific upos\n",
    "        list_of_tokens = [str(word.lemma) for sent in doc.sentences for word in sent.words  if word.upos not in upos]\n",
    "        return list_of_tokens \n",
    "\n",
    "    def remove_stopwords(self, hebrew_data):\n",
    "        # list of stopwords by the spaCy package\n",
    "        word_tokens = list(adv.stopwords['hebrew']) \n",
    "        # Remove stopwords\n",
    "        list_remove_stopwords = [word_token for word_token in hebrew_data if word_token not in word_tokens ]\n",
    "        return list_remove_stopwords\n",
    "\n",
    "    def remove_special_characters(self, hebrew_data): \n",
    "        # Removing html tags\n",
    "        no_html_tags = list( re.sub(r'<[^>]*>', '', i) for i in hebrew_data)\n",
    "        amoji ='[(\\U0001F600-\\U0001F92F|\\U0001F300-\\U0001F5FF|\\U0001F680-\\U0001F6FF|\\U0001F190-\\U0001F1FF|\\U00002702-\\U000027B0|\\U0001F926-\\U0001FA9F|\\u200d|\\u2640-\\u2642|\\u2600-\\u2B55|\\u23cf|\\u23e9|\\u231a|\\ufe0f)]'\n",
    "        no_emoji = list(re.sub(amoji+'+','', i) for i in no_html_tags)\n",
    "        # remove numbers and eng words \n",
    "        no_integers = list( re.sub('[0-9a-zA-Z+]', '', i) for i in no_emoji)\n",
    "        # list of special characters \n",
    "        punctuations = list(string.punctuation)\n",
    "        punctuations.extend([' ','—','\"','־'])\n",
    "        # remove special characters \n",
    "        no_special_characters = [token for token in no_integers if token not in punctuations]\n",
    "        return no_special_characters\n",
    "\n",
    "    def convert_list_to_string(self, hebrew_data):\n",
    "        # convert list to string\n",
    "        return ' '.join(hebrew_data)\n",
    "\n",
    "    def data_clearing(self, df):\n",
    "        df_nlp = df.applymap(self.remove_urls)\n",
    "        nlp_stanza = self.get_syntactic_analysis('stanza', df_nlp)\n",
    "        df_nlp = df_nlp.applymap(nlp_stanza)\n",
    "        df_nlp = df_nlp.applymap(self.get_tokens)\n",
    "        #df_nlp.to_csv('stanza.csv', index = False)\n",
    "        #df_nlp = pd.read_csv('stanza.csv',converters={'text': eval})\n",
    "        df_nlp = df_nlp.applymap(self.remove_stopwords)\n",
    "        df_nlp = df_nlp.applymap(self.remove_special_characters)\n",
    "        df_nlp = df_nlp.applymap(self.convert_list_to_string)\n",
    "        return df_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc6060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:45:32 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2022-05-30 16:45:32,261 | INFO | core.py:112 | __init__ | Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2022-05-30 16:45:32 INFO: Use device: cpu\n",
      "2022-05-30 16:45:32,263 | INFO | core.py:123 | __init__ | Use device: cpu\n",
      "2022-05-30 16:45:32 INFO: Loading: tokenize\n",
      "2022-05-30 16:45:32,264 | INFO | core.py:129 | __init__ | Loading: tokenize\n",
      "2022-05-30 16:45:32 INFO: Loading: mwt\n",
      "2022-05-30 16:45:32,276 | INFO | core.py:129 | __init__ | Loading: mwt\n",
      "2022-05-30 16:45:32 INFO: Loading: pos\n",
      "2022-05-30 16:45:32,303 | INFO | core.py:129 | __init__ | Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  ממש כואב ..... אני בוכה עם המשפחה שלא תדעו עוד...\n",
      "1                                  איש יקר שלנו\\t0\\n\n",
      "2                         כל הכבוד והמון בהצלחה\\t0\\n\n",
      "3  \" תל חי , רובי . בכל העצב הזה היית קרן אור של ...\n",
      "4            נקי כפיים ובר לבב בהצלחה לך ולנו .\\t0\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:45:32 INFO: Loading: lemma\n",
      "2022-05-30 16:45:32,669 | INFO | core.py:129 | __init__ | Loading: lemma\n",
      "2022-05-30 16:45:32 INFO: Loading: depparse\n",
      "2022-05-30 16:45:32,721 | INFO | core.py:129 | __init__ | Loading: depparse\n",
      "2022-05-30 16:45:33 INFO: Done loading processors!\n",
      "2022-05-30 16:45:33,314 | INFO | core.py:179 | __init__ | Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text\n",
      "0                   כאב בכה משפחה תדע צער\n",
      "1                                 איש יקר\n",
      "2                          כבוד מון הצלחה\n",
      "3  תל חי רובי עצב קרן אור תקוה נשיא ישראל\n",
      "4                     נקי כף בר לבב הצלחה\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = HebrewDataProcessing()\n",
    "    df= obj.load_data('data/hebrew_text.tsv')\n",
    "    print(df.head())\n",
    "    df_nlp = obj.data_clearing(df)\n",
    "    df_nlp.to_csv('data/preprocessing_hebrew.csv', index = False, header=False)\n",
    "    print(df_nlp.head())\n",
    "    \n",
    "    def hebrow_summarizer(str_text):\n",
    "        print('Text summary:\\n', summarizer.summarize(str_text))\n",
    "        print('keywords:\\n', keywords.keywords(str_text))\n",
    "        \n",
    "    str_text = df_nlp.to_string()\n",
    "    hebrow_summarizer(str_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b896c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b33efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
